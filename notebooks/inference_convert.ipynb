{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e76329a-e13b-4d94-a20e-4f427cbc39e3",
   "metadata": {},
   "source": [
    "# Инференс и конвертация модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1590159-42fe-4626-9c84-f2eab8531a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e7f4ce-072a-4625-aa90-860c9eb75448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from os import path\n",
    "import sys\n",
    "sys.path.append(path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5d3798-2266-4fcf-9bf8-195b71624228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import onnx\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from typing import Tuple\n",
    "from numpy.typing import NDArray\n",
    "from torch import Tensor\n",
    "\n",
    "from PIL import Image\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import jpeg4py as jpeg\n",
    "import cv2 \n",
    "\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from src.dataset import PlateDataset\n",
    "import src.augmentations as self_augs\n",
    "from src.lightning_module import PlateModule\n",
    "from configs.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4a563725a7d0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(\n",
    "    img: NDArray[float],\n",
    "    mean: Tuple[float, ...] = (0.485, 0.456, 0.406),\n",
    "    std: Tuple[float, ...] = (0.229, 0.224, 0.225),\n",
    "    max_value: int = 255,\n",
    ") -> NDArray[int]:\n",
    "    denorm = albu.Normalize(\n",
    "        mean=[-me / st for me, st in zip(mean, std)],  # noqa: WPS221\n",
    "        std=[1.0 / st for st in std],\n",
    "        always_apply=True,\n",
    "        max_pixel_value=1.0,\n",
    "    )\n",
    "    denorm_img = denorm(image=img)['image'] * max_value\n",
    "    return denorm_img.astype(np.uint8)\n",
    "\n",
    "\n",
    "def tensor_to_cv_image(tensor: Tensor) -> NDArray[float]:\n",
    "    return tensor.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "\n",
    "def onnx_preprocessing(\n",
    "    image,\n",
    "    image_size=(224, 224)\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert numpy-image to array for inference ONNX Runtime model.\n",
    "    \"\"\"\n",
    "\n",
    "    # resize\n",
    "    image = cv2.resize(image.copy(), image_size, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # normalize\n",
    "    mean = np.array((0.485, 0.456, 0.406), dtype=np.float32) * 255.0\n",
    "    std = np.array((0.229, 0.224, 0.225), dtype=np.float32) * 255.0\n",
    "    denominator = np.reciprocal(std, dtype=np.float32)\n",
    "    image = image.astype(np.float32)\n",
    "    image -= mean\n",
    "    image *= denominator\n",
    "\n",
    "    # transpose\n",
    "    image = image.transpose((2, 0, 1))[None]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff763c00-e76d-4e57-bfc4-3c560d0014fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ONNX_MODEL_NAME = '../experiments/exp-2/exp-2_plate-model.onnx'\n",
    "DEVICE = 'cpu'\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374940b898f9a6d2",
   "metadata": {},
   "source": [
    "## Загрузка и тест модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5867ff-cb2d-43fb-9bcb-63d168f1ab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем лучший чекпоинт и формируем модель\n",
    "checkpoint = '/home/dmitriy/Nextcloud/Projects/Proj_courses/DeepSchool/hw-02/model_plate-det/experiments/exp-2/epoch_epoch=72-val_iou=0.857.ckpt'\n",
    "new_model = PlateModule.load_from_checkpoint(checkpoint, config=Config.from_yaml('../configs/config.yaml'))  # указываем явно конфиг, иначе не взлетит\n",
    "new_model.eval()\n",
    "new_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3807cc335a4b1cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверям работу модели\n",
    "transforms = albu.Compose([\n",
    "    albu.Resize(height=512, width=512),\n",
    "    albu.Normalize(\n",
    "        mean=(0.485, 0.456, 0.406),\n",
    "        std=(0.229, 0.224, 0.225),\n",
    "        max_pixel_value=255.0,\n",
    "        always_apply=False,\n",
    "        p=1.0\n",
    "    ),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "image = jpeg.JPEG('../dataset/single_COCO/950.jpg').decode()\n",
    "out = transforms(\n",
    "    image=image\n",
    ")\n",
    "image = out[\"image\"]\n",
    "\n",
    "pr_mask = new_model(image.to(DEVICE).unsqueeze(0))\n",
    "print(pr_mask.shape)\n",
    "pr_mask = (pr_mask.squeeze().cpu().detach().numpy().round())\n",
    "print(pr_mask.shape)\n",
    "\n",
    "test_image = self_augs.denormalize(self_augs.tensor_to_cv_image(image))\n",
    "green_masks = np.zeros(test_image.shape, dtype=np.uint8)\n",
    "valid_area = np.argwhere(pr_mask > 0)\n",
    "green_masks[valid_area[:,0], valid_area[:,1], 1] = 255\n",
    "img_add = cv2.addWeighted(test_image, 0.7, green_masks, 0.3, 0)\n",
    "        \n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "f, axarr = plt.subplots(1, 2) \n",
    "\n",
    "axarr[0].imshow(img_add, cmap='gray', vmin=0, vmax=255)\n",
    "axarr[1].imshow(pr_mask, cmap='gray', vmin=0, vmax=1)\n",
    "# axarr[2].imshow(pr_mask, cmap='gray', vmin=0, vmax=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162d89a5551fcbef",
   "metadata": {},
   "source": [
    "## Переводим модель в ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43437e04-9d2f-4e66-ba72-5d1e960cbb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.rand(1, 3, 512, 512, device=DEVICE)\n",
    "torch.onnx.export(\n",
    "    new_model,\n",
    "    dummy_input,\n",
    "    ONNX_MODEL_NAME,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes = {'input': [0], 'output': [0]}, # динамический батч, но можно и статический\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62857eac-d3a7-4f33-a4e5-1516a8be42e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверяем, что модель конвертнулась успешно\n",
    "onnx_model = onnx.load(ONNX_MODEL_NAME)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(onnx.helper.printable_graph(onnx_model.graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de54ff74b33f1179",
   "metadata": {},
   "source": [
    "## Проверям инференс на ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e37c5d00eff85ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Доступные провайдеры, на которых можно выполнять вычисления\n",
    "print(ort.get_available_providers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f519158b70517c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем сессию\n",
    "\n",
    "# При инициализации сессии можно указать несколько провайдеров. Это может быть полезно, если хотим запускать код\n",
    "# на разных машинах. Например на машине с GPU у нас сработает CUDAExecutionProvider,\n",
    "# а на машине без GPU CPUExecutionProvider\n",
    "providers = [\n",
    "    'CUDAExecutionProvider',\n",
    "    'CPUExecutionProvider',\n",
    "]\n",
    "\n",
    "ort_session = ort.InferenceSession(\n",
    "    ONNX_MODEL_NAME,\n",
    "    providers=providers\n",
    ")\n",
    "\n",
    "print(f'{[input_.name for input_ in ort_session.get_inputs()]}')\n",
    "print(f'{[output_.name for output_ in ort_session.get_outputs()]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aff51978807a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# готовим входной тензор\n",
    "BATCH_SIZE = 1\n",
    "image = jpeg.JPEG('../dataset/single_COCO/950.jpg').decode()\n",
    "onnx_input = onnx_preprocessing(image, image_size=(512, 512))\n",
    "onnx_input = np.concatenate([onnx_input] * BATCH_SIZE)\n",
    "\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: onnx_input}\n",
    "print(list(ort_inputs.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd839b0fce7aea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# выполняем инференс ONNX Runtime\n",
    "ort_outputs = ort_session.run(None, ort_inputs)[0]\n",
    "print(ort_outputs.shape)\n",
    "\n",
    "pr_mask = (ort_outputs.squeeze().round())\n",
    "print(pr_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce275fc-0b3f-4a1f-bde9-f452c11e9d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize\n",
    "test_image = cv2.resize(image.copy(), (512,512), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "green_masks = np.zeros(test_image.shape, dtype=np.uint8)\n",
    "valid_area = np.argwhere(pr_mask > 0)\n",
    "green_masks[valid_area[:,0], valid_area[:,1], 1] = 255\n",
    "img_add = cv2.addWeighted(test_image, 0.7, green_masks, 0.3, 0)\n",
    "        \n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "f, axarr = plt.subplots(1, 2) \n",
    "\n",
    "axarr[0].imshow(img_add, cmap='gray', vmin=0, vmax=255)\n",
    "axarr[1].imshow(pr_mask, cmap='gray', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944e4c39d8dbeb93",
   "metadata": {},
   "source": [
    "инференсы совпали, отлично!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70adea4e5b3ce1",
   "metadata": {},
   "source": [
    "## Обрезаем найденный номер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0a4bbe1ecee065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cropping an image\n",
    "indexes = np.where(green_masks[:, :, 1] == 255)\n",
    "x_min = indexes[0].min()\n",
    "x_max = indexes[0].max()\n",
    "y_min = indexes[1].min()\n",
    "y_max = indexes[1].max()\n",
    "plt.imshow(test_image[x_min:x_max, y_min:y_max, :])"
   ]
  },
  {
   "cell_type": "code",
   "id": "14a01d64b20aa58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw-01",
   "language": "python",
   "name": "hw-01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
